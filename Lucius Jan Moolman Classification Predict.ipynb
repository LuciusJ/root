{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-11T20:29:30.261653Z","iopub.execute_input":"2021-10-11T20:29:30.262669Z","iopub.status.idle":"2021-10-11T20:29:30.279535Z","shell.execute_reply.started":"2021-10-11T20:29:30.262608Z","shell.execute_reply":"2021-10-11T20:29:30.277989Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# utilities\nimport re\nimport string\nimport numpy as np\nimport pandas as pd\n\n# plotting\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# nltk\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\n\n\n# sklearn\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\n\n#for word embedding\nimport gensim\nfrom gensim.models import Word2Vec #Word2Vec is mostly used for huge datasets","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:29:32.080700Z","iopub.execute_input":"2021-10-11T20:29:32.081216Z","iopub.status.idle":"2021-10-11T20:29:33.390572Z","shell.execute_reply.started":"2021-10-11T20:29:32.081169Z","shell.execute_reply":"2021-10-11T20:29:33.389511Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/edsa-climate-change-belief-analysis-2021/train.csv')\ntest = pd.read_csv('../input/edsa-climate-change-belief-analysis-2021/test.csv')\nsample = pd.read_csv('../input/edsa-climate-change-belief-analysis-2021/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:29:34.082541Z","iopub.execute_input":"2021-10-11T20:29:34.083577Z","iopub.status.idle":"2021-10-11T20:29:34.284324Z","shell.execute_reply.started":"2021-10-11T20:29:34.083527Z","shell.execute_reply":"2021-10-11T20:29:34.283260Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:29:36.018377Z","iopub.execute_input":"2021-10-11T20:29:36.018727Z","iopub.status.idle":"2021-10-11T20:29:36.044128Z","shell.execute_reply.started":"2021-10-11T20:29:36.018694Z","shell.execute_reply":"2021-10-11T20:29:36.043168Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:29:36.843181Z","iopub.execute_input":"2021-10-11T20:29:36.843537Z","iopub.status.idle":"2021-10-11T20:29:36.851884Z","shell.execute_reply.started":"2021-10-11T20:29:36.843504Z","shell.execute_reply":"2021-10-11T20:29:36.850956Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#1. WORD-COUNT\ntrain['word_count'] = train['message'].apply(lambda x: len(str(x).split()))\nprint(\"WORD-COUNT_of_positive_tweets\")\nprint(train[train['sentiment']==1]['word_count'].mean()) #Positive tweets\nprint(\"WORD-COUNT_of_Neutral_tweets\")\nprint(train[train['sentiment']==0]['word_count'].mean()) #Neutral tweets\nprint(\"WORD-COUNT_of_Negative_tweets\")\nprint(train[train['sentiment']==-1]['word_count'].mean()) #Negative tweets\n#Disaster tweets are more wordy than the non-disaster tweets\n\n#2. CHARACTER-COUNT\ntrain['char_count'] = train['message'].apply(lambda x: len(str(x)))\nprint(\"CHARACTER-COUNT_of_positive_tweets\")\nprint(train[train['sentiment']==1]['char_count'].mean()) #Positive tweets\nprint(\"CHARACTER-COUNT_of_Neutral_tweets\")\nprint(train[train['sentiment']==0]['word_count'].mean()) #Neutral tweets\nprint(\"CHARACTER-COUNT_of_Negative_tweets\")\nprint(train[train['sentiment']==-1]['word_count'].mean()) #Negative tweets\n#Disaster tweets are longer than the non-disaster tweets\n\n#3. UNIQUE WORD-COUNT\ntrain['unique_word_count'] = train['message'].apply(lambda x: len(set(str(x).split())))\nprint(\"UNIQUE WORD-COUNT_of_positive_tweets\")\nprint(train[train['sentiment']==1]['unique_word_count'].mean()) #Positive tweets\nprint(\"UNIQUE WORD-COUNT_of_Neutral_tweets\")\nprint(train[train['sentiment']==0]['unique_word_count'].mean()) #Neutral tweets\nprint(\"UNIQUE WORD-COUNT_of_Negative_tweets\")\nprint(train[train['sentiment']==0]['unique_word_count'].mean()) #Negative tweets","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:29:37.323233Z","iopub.execute_input":"2021-10-11T20:29:37.324360Z","iopub.status.idle":"2021-10-11T20:29:37.492754Z","shell.execute_reply.started":"2021-10-11T20:29:37.324296Z","shell.execute_reply":"2021-10-11T20:29:37.491535Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#Plotting word-count per tweet\nfig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(15,8))\ntrain_words=train[train['sentiment']==1]['word_count']\nax1.hist(train_words,color='red')\nax1.set_title('Positive tweets')\ntrain_words=train[train['sentiment']==0]['word_count']\nax2.hist(train_words,color='green')\nax2.set_title('Neutral tweets')\ntrain_words=train[train['sentiment']==-1]['word_count']\nax3.hist(train_words,color='blue')\nax3.set_title('Negative tweets')\nfig.suptitle('Words per tweet')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:29:38.148415Z","iopub.execute_input":"2021-10-11T20:29:38.148760Z","iopub.status.idle":"2021-10-11T20:29:38.754385Z","shell.execute_reply.started":"2021-10-11T20:29:38.148729Z","shell.execute_reply":"2021-10-11T20:29:38.753102Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train.info","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:29:38.794780Z","iopub.execute_input":"2021-10-11T20:29:38.795119Z","iopub.status.idle":"2021-10-11T20:29:38.810690Z","shell.execute_reply.started":"2021-10-11T20:29:38.795086Z","shell.execute_reply":"2021-10-11T20:29:38.809515Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:29:39.642725Z","iopub.execute_input":"2021-10-11T20:29:39.643038Z","iopub.status.idle":"2021-10-11T20:29:39.651902Z","shell.execute_reply.started":"2021-10-11T20:29:39.642994Z","shell.execute_reply":"2021-10-11T20:29:39.651183Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train['sentiment'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:29:40.201230Z","iopub.execute_input":"2021-10-11T20:29:40.201632Z","iopub.status.idle":"2021-10-11T20:29:40.211833Z","shell.execute_reply.started":"2021-10-11T20:29:40.201594Z","shell.execute_reply":"2021-10-11T20:29:40.210809Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train['sentiment'].nunique()","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:29:40.848624Z","iopub.execute_input":"2021-10-11T20:29:40.849210Z","iopub.status.idle":"2021-10-11T20:29:40.856539Z","shell.execute_reply.started":"2021-10-11T20:29:40.849172Z","shell.execute_reply":"2021-10-11T20:29:40.855183Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.countplot(x = 'sentiment', data = train)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:29:41.857456Z","iopub.execute_input":"2021-10-11T20:29:41.858678Z","iopub.status.idle":"2021-10-11T20:29:42.102265Z","shell.execute_reply.started":"2021-10-11T20:29:41.858620Z","shell.execute_reply":"2021-10-11T20:29:42.100860Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train['message'] = train['message'].str.lower()\ntrain['message'].tail()","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:29:42.548187Z","iopub.execute_input":"2021-10-11T20:29:42.548499Z","iopub.status.idle":"2021-10-11T20:29:42.580320Z","shell.execute_reply.started":"2021-10-11T20:29:42.548469Z","shell.execute_reply":"2021-10-11T20:29:42.579234Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\nsubs_url = r'url-web'\ntrain['message'] = train['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:29:43.449850Z","iopub.execute_input":"2021-10-11T20:29:43.451030Z","iopub.status.idle":"2021-10-11T20:29:43.520386Z","shell.execute_reply.started":"2021-10-11T20:29:43.450973Z","shell.execute_reply":"2021-10-11T20:29:43.519101Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def stopword(string):\n    a= [i for i in string.split() if i not in stopwords.words('english')]\n    return ' '.join(a)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:29:44.353121Z","iopub.execute_input":"2021-10-11T20:29:44.353444Z","iopub.status.idle":"2021-10-11T20:29:44.358610Z","shell.execute_reply.started":"2021-10-11T20:29:44.353413Z","shell.execute_reply":"2021-10-11T20:29:44.357808Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#3. LEMMATIZATION\n# it is the process of reducing the word to its base form\n\n# Initialize the lemmatizer\nwl = WordNetLemmatizer()\n \n# This is a helper function to map NTLK position tags\n# Full list is available here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\ndef get_wordnet_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\n# Tokenize the sentence\ndef lemmatizer(string):\n    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n    return \" \".join(a)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:37:17.523097Z","iopub.execute_input":"2021-10-11T20:37:17.523428Z","iopub.status.idle":"2021-10-11T20:37:17.533629Z","shell.execute_reply.started":"2021-10-11T20:37:17.523398Z","shell.execute_reply":"2021-10-11T20:37:17.532285Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"\n#FINAL PREPROCESSING\ndef finalpreprocess(string):\n    return lemmatizer(stopword(string))\n\ntrain['clean_message'] = train['message'].apply(lambda x: finalpreprocess(x))\ntrain = train.drop(columns = ['word_count','char_count','unique_word_count'])\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:37:22.282829Z","iopub.execute_input":"2021-10-11T20:37:22.283826Z","iopub.status.idle":"2021-10-11T20:38:36.413784Z","shell.execute_reply.started":"2021-10-11T20:37:22.283772Z","shell.execute_reply":"2021-10-11T20:38:36.412268Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# create Word2vec model\n#here words_f should be a list containing words from each document. say 1st row of the list is words from the 1st document/sentence\n#length of words_f is number of documents/sentences in your dataset\ntrain['clean_message_tok']=[nltk.word_tokenize(i) for i in train['clean_message']] #convert preprocessed sentence to tokenized sentence\nmodel = Word2Vec(train['clean_message_tok'],min_count=1)  #min_count=1 means word should be present at least across all documents,\n#if min_count=2 means if the word is present less than 2 times across all the documents then we shouldn't consider it\n\n\nw2v = dict(zip(model.wv.index_to_key, model.wv.vectors))  #combination of word and its vector\n\n#for converting sentence to vectors/numbers from word vectors result by Word2Vec\nclass MeanEmbeddingVectorizer(object):\n    def __init__(self, word2vec):\n        self.word2vec = word2vec\n        # if a text is empty we should return a vector of zeros\n        # with the same dimensionality as all the other vectors\n        self.dim = len(next(iter(word2vec.values())))\n\n    def fit(self, X, y):\n        return self\n\n    def transform(self, X):\n        return np.array([\n            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n                    or [np.zeros(self.dim)], axis=0)\n            for words in X\n        ])\n","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:39:30.045383Z","iopub.execute_input":"2021-10-11T20:39:30.045700Z","iopub.status.idle":"2021-10-11T20:39:37.387889Z","shell.execute_reply.started":"2021-10-11T20:39:30.045668Z","shell.execute_reply":"2021-10-11T20:39:37.386583Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#SPLITTING THE TRAINING DATASET INTO TRAINING AND VALIDATION\n \n# Input: \"reviewText\", \"rating\" and \"time\"\n# Target: \"log_votes\"\nX_train, X_val, y_train, y_val = train_test_split(train[\"clean_message\"],\n                                                  train[\"sentiment\"],\n                                                  test_size=0.2,\n                                                  shuffle=True)\nX_train_tok= [nltk.word_tokenize(i) for i in X_train]  #for word2vec\nX_val_tok= [nltk.word_tokenize(i) for i in X_val]      #for word2vec\n\n#TF-IDF\n# Convert x_train to vector since model can only run on numbers and not words- Fit and transform\ntfidf_vectorizer = TfidfVectorizer(use_idf=True)\nX_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) #tfidf runs on non-tokenized sentences unlike word2vec\n# Only transform x_test (not fit and transform)\nX_val_vectors_tfidf = tfidf_vectorizer.transform(X_val) #Don't fit() your TfidfVectorizer to your test data: it will \n#change the word-indexes & weights to match test data. Rather, fit on the training data, then use the same train-data-\n#fit model on the test data, to reflect the fact you're analyzing the test data only based on what was learned without \n#it, and the have compatible\n\n\n#Word2vec\n# Fit and transform\nmodelw = MeanEmbeddingVectorizer(w2v)\nX_train_vectors_w2v = modelw.transform(X_train_tok)\nX_val_vectors_w2v = modelw.transform(X_val_tok)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:39:48.951587Z","iopub.execute_input":"2021-10-11T20:39:48.951928Z","iopub.status.idle":"2021-10-11T20:39:54.898745Z","shell.execute_reply.started":"2021-10-11T20:39:48.951893Z","shell.execute_reply":"2021-10-11T20:39:54.897599Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n\nlr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\nlr_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n\n#Predict y value for test dataset\ny_predict = lr_tfidf.predict(X_val_vectors_tfidf)\ny_prob = lr_tfidf.predict_proba(X_val_vectors_tfidf)[:,1]\n \n\nprint(classification_report(y_val,y_predict))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:39:54.900555Z","iopub.execute_input":"2021-10-11T20:39:54.901140Z","iopub.status.idle":"2021-10-11T20:39:55.533812Z","shell.execute_reply.started":"2021-10-11T20:39:54.901096Z","shell.execute_reply":"2021-10-11T20:39:55.532844Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"#FITTING THE CLASSIFICATION MODEL using Naive Bayes(tf-idf)\n#It's a probabilistic classifier that makes use of Bayes' Theorem, a rule that uses probability to make predictions based on prior knowledge of conditions that might be related. This algorithm is the most suitable for such large dataset as it considers each feature independently, calculates the probability of each category, and then predicts the category with the highest probability.\n\nnb_tfidf = MultinomialNB()\nnb_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n\n#Predict y value for test dataset\ny_predict = nb_tfidf.predict(X_val_vectors_tfidf)\ny_prob = nb_tfidf.predict_proba(X_val_vectors_tfidf)[:,1]\n \n\nprint(classification_report(y_val,y_predict))\nprint('Confusion Matrix:',confusion_matrix(y_val, y_predict))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:39:57.604733Z","iopub.execute_input":"2021-10-11T20:39:57.604998Z","iopub.status.idle":"2021-10-11T20:39:57.636581Z","shell.execute_reply.started":"2021-10-11T20:39:57.604972Z","shell.execute_reply":"2021-10-11T20:39:57.635342Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"#FITTING THE CLASSIFICATION MODEL using Logistic Regression (W2v)\nlr_w2v=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\nlr_w2v.fit(X_train_vectors_w2v, y_train)  #model\n\n#Predict y value for test dataset\ny_predict = lr_w2v.predict(X_val_vectors_w2v)\ny_prob = lr_w2v.predict_proba(X_val_vectors_w2v)[:,1]\n \n\nprint(classification_report(y_val,y_predict))\nprint('Confusion Matrix:',confusion_matrix(y_val, y_predict))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:39:59.711221Z","iopub.execute_input":"2021-10-11T20:39:59.711528Z","iopub.status.idle":"2021-10-11T20:40:06.869825Z","shell.execute_reply.started":"2021-10-11T20:39:59.711498Z","shell.execute_reply":"2021-10-11T20:40:06.867093Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"df_test = test\ndf_test['clean_message'] = test['message'].apply(lambda x: finalpreprocess(x)) #preprocess the data\nX_test=df_test['clean_message'] \nX_vector=tfidf_vectorizer.transform(X_test) #converting X_test to vector\ny_predict = lr_tfidf.predict(X_vector)      #use the trained model on X_vector\ny_prob = lr_tfidf.predict_proba(X_vector)[:,1]\ndf_test['sentiment']= y_predict\nprint(df_test.head())","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:41:02.560345Z","iopub.execute_input":"2021-10-11T20:41:02.560693Z","iopub.status.idle":"2021-10-11T20:41:51.293459Z","shell.execute_reply.started":"2021-10-11T20:41:02.560658Z","shell.execute_reply":"2021-10-11T20:41:51.292196Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"test[['tweetid', 'sentiment']].to_csv('testsubmission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T20:41:51.295646Z","iopub.execute_input":"2021-10-11T20:41:51.296005Z","iopub.status.idle":"2021-10-11T20:41:51.331583Z","shell.execute_reply.started":"2021-10-11T20:41:51.295960Z","shell.execute_reply":"2021-10-11T20:41:51.330378Z"},"trusted":true},"execution_count":28,"outputs":[]}]}